\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{graphicx}

\usepackage{float}

\graphicspath{{home/yash/Downloads/latex/}}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}    


%http://www.docs.is.ed.ac.uk/skills/documents/3722/3722-2014.pdf
%https://stackoverflow.com/questions/3175105/inserting-code-in-this-latex-document-with-indentation
%https://tex.stackexchange.com/questions/74279/how-to-add-bullets-to-description-lists
%https://tex.stackexchange.com/questions/153506/insert-a-new-line-without-newline-command
%https://tex.stackexchange.com/questions/261693/latex-element-of-with-two-strokes-%E2%8B%B9
%https://tex.stackexchange.com/questions/101918/how-to-use-maths-symbols-in-text-mode
%https://en.wikipedia.org/wiki/Probability_mass_function
%https://www.overleaf.com/learn/latex/Errors/%60!h'%20float%20specifier%20changed%20to%20%60!ht'
%https://www.overleaf.com/learn/latex/Questions/How_do_I_insert_an_image_at_a_specific_point_in_the_document%3F
%https://en.wikibooks.org/wiki/LaTeX/Floats,_Figures_and_Captions
%https://www.alz.org/alzheimers-dementia/what-is-alzheimers

\title{Predicting Alzheimer's}
\author{Yash Karandikar}
\date{August 2019}

\begin{document}

\maketitle

\section{Big Question}
\large{How effective are machine learning and statistical algorithms at classifying whether patients will contract Alzheimer’s?}

\section{Abstract}
Abstract

\section{Intro}

%-use Kaggle data set “MRI & Alzheimer’s”, specifically longitudinal study

\section{Machine learning model}

To analyze the problem with machine learning, I used Python with assorted data-analytical packages (namely numpy and pandas) as well as graphical packages (matplotlib and seaborn). The Kaggle data set I used is entitled "MRI and Alzheimer's."\newline
    
After extracting the data from the .csv file, I fed all of the related factors (such as age and mental state) and metrics (like estimated brain volume and clinical dementia rating) into a neural network with appropriate weights and biases. For instance, age and clinical rating are potential causes of dementia, while mental state and brain volume tend to be products of it.\newline
    
In the following code chunks, I:
\begin{description}
\large{\item[$\bullet$] find the mean of each array to standardize the sigmoid input}
\end{description}

\begin{lstlisting}
df['Age'] = 100 - df['Age'] # age is inversely proportional to remission

# take the mean to standardize the final array and include all of the factors
ageMean = df['Age'].mean()
sesMean = df['Socio-economic status'].mean()
mseMean = df['Mental state examination'].mean()
estivMean = df['Estimated total intracranial volume'].mean()
meanArray = [ageMean, sesMean, mseMean, estivMean]

# standardize the sigmoid function accordingly, to return output
def standardizedSigmoid(x, mean):
    return x / mean
\end{lstlisting}

\begin{description}
\large{\item[$\bullet$] define the neural network and its feed forward function}
\end{description}

\begin{lstlisting}
# define neural network
patientArray = [] # used as global

class HealthcareAnalysis:
    def __init__(self, weight):
        self.weight = weight

    def feedForward(self, inputHealth, inputWeights, meanArray):
        global patientArray

        # iterate through the parameters for health
        # apply weights with matrix multiplication, not dot product
        total = 0
        for i in range(len(inputHealth)):
            inputHealth[i] = inputHealth[i] * inputWeights[i]
            total += inputHealth[i]

        # find standardized total, not just raw sum based on weights
        for i in range(0, 3):
            standardTotal = standardizedSigmoid(total, meanArray[i])

            for j in range(len(meanArray)):
                # ensure that output is between 0 and 1
                if 10 < standardTotal[j] < 100:
                    standardTotal = standardTotal / 100
                elif standardTotal[j] > 100:
                    standardTotal = standardTotal / 1000
            patientArray = standardTotal
        return standardTotal
\end{lstlisting}
    
\begin{description}
\large{\item[$\bullet$] and activate the neural network for its output}
\end{description}

\begin{lstlisting}
# set up all parameters in master array
healthArray = np.array([age, socioEconomicStatus, mentalStateExamination,
                        estimatedTotalIntracranialVolume])

"""
-set health weights up; age impacts dementia with a positive correlation
-socioeconomic status can impact treatment, but doesn't fundamentally change the disease
-deteriorated mental state also has a positive correlation with dementia, more so than age
-total intracranial volume is a physiological sign of dementia progression, significant
"""

ageWeight = [1]
ses = [0.75]
mse = [1.25]
estiv = [1.25]

# the weights are somewhat arbitrary, but add precision
healthWeights = np.array([ageWeight, ses, mse, estiv])

# initialize class instances of neural network
healthData = HealthcareAnalysis(healthArray)
healthCalculation = healthData.feedForward(healthArray, healthWeights, meanArray)
\end{lstlisting}

These lines of code funnel the variables which lead to Alzheimer's into the black-box nature of the neural net, which spits out a value x $\in$ [0, 1] depending on likelihood of the disease.
    
%ML model uses a neural network to gauge specific factors given by Kaggle data set (neural networks bring in a variety of factors and squeeze them through input layers, which spit out a classification that we can use to theoretically predict which patients have the disease)

\section{Statistical model}

I also analyzed the problem using statistics. After graphing the data with the variables on the x-axis and the diagnoses on the y-axis (in a similar format to a machine-learning classification problem), I noticed that the data was normally distributed. As a result, I used differing normal models to visualize the problem.

\subsection{Normal probability plot}
It is worth noting the utility of normal probability plots, which reveal to what extent the data is normally distributed through the graph's linearity. Probability plots have the "theoretical quantities" on the x-axis (which are the abstract numbers from mathematical assumptions) and the "sample quantities" on the y-axis (which are the empirical data used in the experiment).\newline

Probability plots are useful when one wants to find out what model would best fit the data. There are many options available for normal distributions (such as t-distributions) as well as scaling options (like using z-scores) that make probability plots useful for initial analysis.\newline

Figure 1 is an example of a normal probability plot which uses the data set of estimated brain volume for patients. Because the line is so straight, it is implied that brain volume is normally distributed. This observation is consistent with the data, since some patients will have lesser volume (associated with Alzheimer's) and some will have greater volume (associated with remission or simply not having the disease).

\begin{figure}[H]
    \centering
    \includegraphics[width=0.5\textwidth]{"prob_plot_etiv"}
    \caption{example of probability plot}
\end{figure}

\subsection{Probability mass function}
The probability mass function (for normally distributed data) graphs the element on the x-axis and the probability of the element occurring on the y-axis. It somewhat resembles parabolic motion, and the integral from the lower to upper bound always equals 1. This is due to the property that any given element is binary (i.e. occurs or doesn't), so all values' probabilities given mutual exclusion happen independently of each other and guarantee all total possibilities.\newline

Figures 2, 3, and 4 use the probability mass function to assess age, mental state, and brain volume. As the data becomes increasingly normally distributed, the data points get closer and y-value distances get smaller proportionately.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{"age_j"}
    \caption{probability mass function: age}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{"mse_j"}
    \caption{probability mass function: mental state evaluation}
\end{figure}

Notice that the mental state evaluation graph is slightly less dense than the age graph. This is because of the relative discrepancy in how they are normally distributed. Age, in other words, is more normally distributed than mental state.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.65\textwidth]{"etiv_j"}
    \caption{probability mass function: estimated brain volume}
\end{figure}

Estimated total intracranial volume (the aforementioned "etiv" in the graph) is incredibly normally distributed, which explains the density of the probability mass function. This is expected behavior because the data set is a mixed bag of patients with and without Alzheimer's, rounded out by those for whom the disease went into remission.

%-statistical model uses probability mass function to perform a binomial distribution, which in turn graphs the normal distribution of normal/demented patients as an almost quadratic-looking function

%-show binomial function graphs + seaborn/matplotlib output

\section{Effectiveness of model}

\section{Analysis}
Though the approach of using machine learning algorithms and statistical concepts is relatively effective, it simply doesn't possess the scope necessary to tackle Alzheimer's diagnosis at a more broad level. My model takes certain important factors into account (such as age and socio-economic status) as well as potential implications of diagnosis (like brain volume and mental state) which are good predictors post-hoc of having contracted Alzheimer's.\newline

However, there are many more factors which are harder to statistically gather or quantify. For instance, suppose a person susceptible to Alzheimer's has multiple concussions at a young age. That person is higher at risk for Alzheimer's or other neurological diseases, but modern-day statistics simply don't possess the complexity needed to take that into account when using similar analysis as I have performed. Concussion rate, genetics and general health/exercise/diet are equally important factors, to name a few. There are also many other potential predictors of Alzheimer's like beta-amyloid in the system and buildup of tau inside cells, among others.\newline

Though the sheer amount of complexity that biology and neurology afford the human body is immense, the computational power at our disposal is only increasing. Machine learning (similar analysis to Section 3, but on a much grander scale and with far superior algorithmic processing) is a promising field to break down these diseases, because machines can much more easily handle the massive loads of data that human doctors cannot. In addition, the optimization of algorithms and heuristics to save time tend to proportionally increase more than the relative costs (whether financial, logistical, or computational), which is heartening for those seeking a cure.

\section{Conclusion}

\end{document}
