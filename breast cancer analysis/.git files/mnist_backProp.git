# CNN model for the structure of this code: https://victorzhou.com/blog/intro-to-cnns-part-2/
# trained, part 2

# yield keyword: https://stackoverflow.com/questions/231767/what-does-the-yield-keyword-do
# np.shape: https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html
# np.zeroes: https://docs.scipy.org/doc/numpy/reference/generated/numpy.zeros.html
# np.shape needs (()) - https://stackoverflow.com/questions/5446522/data-type-not-understood
# _ as a throwaway Pythonic variable: https://stackoverflow.com/questions/5893163/what-is-the-purpose-of-the-single-underscore-variable-in-python
# np.flatten() - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.flatten.html
# np.argmax() - provides the index for the maximum values along an axis - https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html
# enumerate() - https://docs.python.org/3/library/functions.html#enumerate
# %s and %d - https://stackoverflow.com/questions/4288973/whats-the-difference-between-s-and-d-in-python-string-formatting/48660475

# continue - https://docs.python.org/2.0/ref/continue.html
# pass vs continue - https://stackoverflow.com/questions/9483979/is-there-a-difference-between-continue-and-pass-in-a-for-loop-in-python
# np.newaxis (n+1 dimensional array) - https://stackoverflow.com/questions/29241056/how-does-numpy-newaxis-work-and-when-to-use-it
# @ with respect to matrices/transposes - matrix multiplication - https://stackoverflow.com/questions/6392739/what-does-the-at-symbol-do-in-python

import numpy as np
import mnist

class Conv3x3:
    # a convolution layer using 3x3 filters

    def __init__(self, num_filters):
        self.num_filters = num_filters

        # filters is a 3D array with dimensions (num_filters, 3, 3)
        # we know two of the three dimensions' numerical values, divide by 9 to account for that
        self.filters = np.random.randn(num_filters, 3, 3) / 9

    def iterate_regions(self, image):
        # generates all possible 3x3 image regions, similar to ij iteration in loops
        # the image provided must be a 2D numpy array
        h, w = image.shape # height and width

        # index is h-2, w-2 because region is 3x3
        # we want only a sliver at a time for the network to process
        for i in range(h - 2):
            for j in range(w - 2):
                im_region = image[i: (i+3), j:(j+3)] # uses a Pythonic generator to create an iterator on the fly
                yield im_region, i, j

    def forwardProp(self, input):
        # performs forward propagation of the convolutional layer, going through the network with ij iteration
        # input is 2D np.array, output is 3D np.array with dimensions (h, w, num_filters)

        self.last_input = input

        h, w = input.shape

        # returns a new 2D array of zeroes with parameters (h-2, w-2) and data type self.num_filters (int)
        output = np.zeros((h-2, w-2, self.num_filters)) # 2D array, two parentheses

        # take generator from self.iterate_regions
        # number of regions * filters per region = np.sum for the convolutional neural network to compute
        for im_region, i, j in self.iterate_regions(input):
            # axis(1,2) produces a 1D array of length num_filters where each element has its respective conv result
            output[i, j] = np.sum(im_region * self.filters, axis=(1,2))
        return output

    def backProp(self, dL_dOutput, learn_rate):
        # dL/dOut is the loss gradient, learn_rate is a given float
        dL_dFilters = np.zeros(self.filters.shape)

        for im_region, i, j in self.iterate_regions(self.last_input):
            for f in range(self.num_filters): # f = filters
                dL_dFilters[f] += dL_dOutput[i, j, f] * im_region

        # update filters by using stochastic gradient descent
        self.filters -= learn_rate * dL_dFilters

        # don't return any value because Conv3x3 is the first layer in the CNN
        # otherwise return the loss gradient like any other CNN layer
        return None
"""
train_images = mnist.train_images()
train_labels = mnist.train_labels()

conv_nn = Conv3x3(8)
output = conv_nn.forwardProp(train_images[0])
print(output.shape)
"""

# pool to reduce the amount of noise in the neural network; each pixel will have relatively similar information
# to condense the information, relatively close pixels are pooled (averaged or max/min) to create a smaller output
# pooling proportionately reduces h, w of the input by poolSize (h*w*l / poolSize)

class MaxPoolSize2:
    # max pooling layer using poolSize = 2

    def iterate_regions(self, image):
        # generates non-overlapping 2x2 image regions to pool (put together very similar regions for concision in data)
        # image is a 2D numpy array
        h, w, _ = image.shape # _ is just a placeholder variable
        new_h = h // 2 # cannot have decimals, need integers for pooling
        new_w = w // 2

        # no need to subtract two from range(), need to cover all possible pixels
        for i in range(new_h):
            for j in range(new_w):
                # use another Pythonic generator for on-the-fly region examination
                im_region = image[(i*2): (i*2 + 2), (j*2): (j*2 + 2)]
                yield im_region, i, j

    def forwardProp(self, input):
        # goes through the max pool layer
        # input is 3D np.array (h, w, num_filters), output is 3D np.array (new_h, new_w, num_filters)

        self.last_input = input

        h, w, num_filters = input.shape # use np.shape to instantiate rather than collect parameter length from
        output = np.zeros((h // 2, w // 2, num_filters))

        for im_region, i, j in self.iterate_regions(input):
            # use the max, not the average, to pool
            # axis = (0,1) to only iterate h, w and not num_filters
            output[i, j] = np.amax(im_region, axis=(0,1))
        return output

    def backProp(self, dL_dOutputs):
        # perform backpropagation for the maxpool layer
        # return the loss gradient, i.e. how the network should adjust weights to reach 0 with gradient descent

        dL_dInputs = np.zeros(self.last_input.shape)

        for im_region, i, j in self.iterate_regions(self.last_input):
            h, w, f = im_region.shape # height, weight, filter value
            amax = np.amax(im_region, axis=(0,1)) # axis(0,1) returns the max from each axis

            for i2 in range(h):
                for j2 in range(w):
                    for f2 in range(f):
                        # if this pixel is the max value, assign the pixel to the gradient and return it
                        # if the pixel is not the max, then assign 0 to the value in backpropagation
                            # because it does not improve the weights in any way and we lose information
                            # e.g. d/dx(x^2) = 2x, but int(2x)dx = x^2 + C, the + C represents the lost information
                        if im_region[i2, j2, f2] == amax[f2]:
                            # i1, j1, f1 represent the original pooling process in forward propagation
                            # i2, j2, f2 scale the max values backwards and place them in the larger, noisier matrix
                            dL_dInputs[i * 2 + i2, j * 2 + j2, f2] = dL_dOutputs[i, j, f2]

        return dL_dInputs

"""
train_images = mnist.train_images()
train_labels = mnist.train_labels()

conv_nn = Conv3x3(8)
pool_nn = MaxPoolSize2()

output = conv_nn.forwardProp(train_images[0])
output = pool_nn.forwardProp(output)
print(output.shape)
"""

# softmax is an equation which takes the pool_nn layers and further condenses the data
# it quantifies how sure we are of the prediction, using a quantity called loss
# L = -ln(p_c), where p_c is the predicted probability that the neural network provides
# to minimize L, -ln(p_c) = 0, which is true when p_c = 1 (i.e. perfect certainty)

class Softmax:
    # create a fully connected layer with the softmax equation

    def __init__(self, input_len, nodes):
        # divide by input_len to reduce variance for the dimensions the network concretely knows
        self.weights = np.random.randn(input_len, nodes) / input_len # create array with specific shape/values of nodes
        self.biases = np.zeros(nodes) # only 1D np.array for biases

    def forwardProp(self, input):
        # goes through the softmax layer
        # input is any array with any dimensions (generalized)
        # output is 1D array with probability of the network's predictions

        # capture input shape before flattening
        self.last_input_shape = input.shape

        input = input.flatten()
        self.last_input = input # capture input shape after flattening

        # from the specific network weights, get information about the input_len and nodes that are generalized
        input_len, nodes = self.weights.shape

        totals = np.dot(input, self.weights) + self.biases # dot product + biases to shift
        self.last_totals = totals

        exp = np.exp(totals) # use e^x for softmax
        divide_exp = exp / np.sum(exp, axis=0) # axis=0 indicates a horizontal movement through the array
        return divide_exp

    def backProp(self, dL_dOut, learn_rate):
        # dL/dOut is the derivative of the loss with respect to the outputs
        # what does dL/dOut mean? dL/dOut tells the network how to better the weights to improve the next forwardProp
        # if dL/dOut is 0, then c (the prediction) is wrong, the network has no new information to improve the weights
        # but if dL/dOut is not 0 (-ln(x) -> -1/x), then c is right, the network has new information and better weights

        # performs back propagation of the softmax layer (goes through network in reverse + updates weights)
        # returns loss gradient dL/dOut, which tells the network how to modify and improve the weights

        # stochastic gradient descent - use algebra as a proxy for calculus
        # it uses a variable learn_rate, which is a float

        # only 1 element of dL/dOut is non-zero (i.e. the classification is right)
        for i, gradient in enumerate(dL_dOut):
            if gradient == 0:
                continue

            # e ^ (totals)
            total_exp = np.exp(self.last_totals) # raise e to the dot products calculated in forward propagation

            # define sum of all e^totals
            s = np.sum(total_exp)

            # define (gradients of output[i]) / (totals)
            dOut_dt = -total_exp[i] * total_exp / (s**2)  # chain rule
            dOut_dt[i] = total_exp[i] * (s - total_exp[i]) / (s**2) # quotient rule

            # calculate weights, biases, and input gradients
            # t = w * input + b, using partial derivatives (i.e. everything else is constant, more variables than x/y)
            # dt/dw = input
            # dt/db = 1
            # dt/dInputs = w

            dt_dw = self.last_input
            dt_db = 1
            dt_dInputs = self.weights # w = weights

            # chain rule for derivative of loss with respect to total
            dL_dt = gradient * dOut_dt

            # chain rule for weights, biases, inputs
            dL_dw = dt_dw[np.newaxis].T @ dL_dt[np.newaxis]
            dL_db = dL_dt * dt_db
            dL_dInputs = dt_dInputs @ dL_dt

            # update the weights and biases - use minus because gradient descent tends error to 0
            self.weights -= learn_rate * dL_dw
            self.biases -= learn_rate * dL_db

            # reshape the output to fit the actual data such that the network can implement backpropagation changes
            return dL_dInputs.reshape(self.last_input_shape)

# only use the first 1000 testing examples
train_images = mnist.train_images()[:1000]
train_labels = mnist.train_labels()[:1000]
test_images = mnist.test_images()[:1000]
test_labels = mnist.test_labels()[:1000]

conv_nn = Conv3x3(8) # 28x28x1 -> 26x26x8, adding volume by num_filters
pool_nn = MaxPoolSize2() # 26x26x8 -> 13x13x8, reducing size by pooling
softmax = Softmax(13 * 13 * 8, 10) # 13x13x8 -> 10, end up with 10 elements

def forward_cnn(image, label):
    # complete forward propagation through the whole CNN
    # calculates accuracy and cross-entropy loss
    # image is a 2D np.array, label is a digit we want the final number of elements to be

    # transform the image from [0, 255] (by 2^8 - 1) to [-0.5, 0.5] for convenience
    output = conv_nn.forwardProp((image / 255) - 0.5) # add volume with num_filters
    output = pool_nn.forwardProp(output) # reduce and pool
    output = softmax.forwardProp(output) # use the softmax equation

    # calculate cross-entropy loss and accuracy, use np.log() for ln(x)
    loss = -1 * np.log(output[label]) # output[label] accesses the desired index from the 1D prediction np.array

    if np.argmax(output) == label: # is the network's prediction genuinely true?
        accuracy = 1
    else:
        accuracy = 0
    return output, loss, accuracy

def train(im, label, learn_rate=0.005):
    # completes a full training step, forward and back
    # returns cross-entropy loss and accuracy
        # image is a 2D numpy array, label is a digit

    # forwards
    out, loss, accuracy = forward_cnn(im, label)

    # calculate the initial gradient
    gradient = np.zeros(10)
    gradient[label] = -1 / out[label]

    # backwards
    gradient = softmax.backProp(gradient, learn_rate)
    gradient = pool_nn.backProp(gradient)
    gradient = conv_nn.backProp(gradient, learn_rate)

    return loss, accuracy

print("CNN initialized")

# train for 3 epochs
for epoch in range(3):
    print('Epoch %s' % epoch)

    # shuffle the data
    permutation = np.random.permutation(len(train_images))
    train_images = train_images[permutation]
    train_labels = train_labels[permutation]

    # train
    loss = 0
    num_correct = 0

    # enumerate() is a built-in Pythonic function that returns a tuple with the list index number and element
    for i, (im, label) in enumerate(zip(test_images, test_labels)):

        # print every 100 steps
        if i % 100 == 99:
            # %s, %d are used to format strings when left as placeholder variables
            print('[Step %d] Past 100 steps: Average loss: %.3f | Accuracy: %d%%' % (i+1, loss / 100, num_correct))

            # reset values every 100 steps
            loss = 0
            num_correct = 0

        l, accuracy = train(im, label)
        loss += l
        num_correct += accuracy

print('Testing CNN')
loss = 0
num_correct = 0
for im, label in zip(test_images, test_labels):
    _, l, accuracy = forward_cnn(im, label)
    loss += l
    num_correct += accuracy

num_tests = len(test_images)
print("Test loss: ", loss / num_tests, "%")
print("Test accuracy: ", num_correct / num_tests, "%")
