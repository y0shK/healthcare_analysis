# code model:
# https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python

# d-types: https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html
# np.amax: https://docs.scipy.org/doc/numpy/reference/generated/numpy.amax.html
# np.random.randn: https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html
# array[:-n or None]: https://stackoverflow.com/questions/15715912/remove-the-last-n-elements-of-a-list
# itertools and product: https://stackoverflow.com/questions/6499327/the-pythonic-way-to-generate-pairs

# define parameters for the neural network - two random factors + diagnosis
nn_parameter1 = factors[parameterStrings[parameter1]]
nn_parameter2 = factors[parameterStrings[parameter2]]
nn_parameter_output = factors['Diagnosis']

# remove elements from the parameters so that MemoryError does not occur
# too many elements increase computation time significantly (especially with itertools.product)
# arbitrarily cut 369 elements, leave 200
n = 369
nn_parameter1_short = nn_parameter1[:-n or None] # list splicing to get rid of all indices after n, or None if n = 0
nn_parameter2_short = nn_parameter2[:-n or None]
nn_parameter_output_short = nn_parameter_output[:-n or None]

"""
Approach:
Use itertools to create ordered pairs from two separate lists
e.g. a = [], b = [], itertools.product(a[i], b[j])

Comments:
-Create empty arrays to append each ordered itertools pair
-Add a counter to break from the loop upon appending enough pairs, such that len(Y) = len(Y-hat) for the neural network
"""
input_nn = []
i_j_count = 0

for i, j in itertools.product(nn_parameter1_short, nn_parameter2_short):
    input_nn.append([i, j]) # append ordered pair as a list, not a dictionary
    i_j_count += 1
    if i_j_count == 200: # based upon the prior n cut of the data: i_j_count + n = len(parameter) = 569
        break

# no need to use counter for output data, because it is pre-split
output_of_data = []

for i in nn_parameter_output_short:
    output_of_data.append([nn_parameter_output_short[i]])

# make the arrays np.arrays (which are essentially matrix arrays)
# this allows for matrix multiplication with dot products in the neural network
input_nn = np.array(input_nn)
output_of_data = np.array(output_of_data)

class neuralNetwork(object):
    def __init__(self):
        # parameters - amount of items in each layer (we use 569 to match the size of the data provided)
        self.inputSize = 2
        self.outputSize = 1
        self.hiddenSize = 3

        # use random.randn to return a random value from the provided standard normal curve

        # define weights with respect to their input & hidden sizes
        self.w1 = np.random.randn(self.inputSize, self.hiddenSize) # input to hidden
        self.w2 = np.random.randn(self.hiddenSize, self.outputSize) # hidden to output

    def forwardProp(self, X):
        self.z1 = np.dot(X, self.w1) # dot product (number) of input and first random 3x2 weights
        self.z2 = self.sigmoid(self.z1) # activation function
        self.z3 = np.dot(self.z2, self.w2) # dot product of hidden layer and second random 3x2 weights
        output = self.sigmoid(self.z3) # final activation function to return
        return output

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoidPrime(self, x):
        return x * (1 - x)

    def backProp(self, X, Y, output):
        # how different are the actual and predicted values?
        # use y - y-hat by convention
        self.outputError = Y - output

        # use gradient descent to move down the sigmoid curve
        # goal: get closer to absolute minimum, 0
        # use the derivative (change in slope) to describe a small, 'local' change on the gradient descent algorithm
        self.outputDelta = self.outputError * self.sigmoidPrime(output)

        # https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html
        # np_array.T is the transpose of np_array
        self.z2_error = self.outputDelta.dot(self.w2.T)

        # same process of derivative and applying gradient descent for the hidden layer
        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)

        # how are we changing the weights?
        # take the transpose of the input array - X.T
        # take its dot product with the hidden layer delta (how is slope changing on the curve? i.e. gradient descent)
        # this dot product is the change in weights to get to 0 (absolute minimum)
        # transpose(X) * change in hidden layer slope = how the weight needs to change to get the algorithm to the min
        # this product is a change (or derivative), so it is added (not overridden) to the original weight
        self.w1 += X.T.dot(self.z2_delta) # input to hidden
        self.w2 += self.z2.T.dot(self.outputDelta) # hidden to output

    def train_network(self, X, Y):
        # implement the neural network algorithm - calculate, analyze error, repeat
        # this function runs in a loop so the network gets new information more than once
        output = self.forwardProp(X)
        self.backProp(X, Y, output)

nn = neuralNetwork()

# create loss array to ensure that the network is more accurate with each iteration
lossArray = []

for i in range(0, 5): # arbitrarily run 5 iterations
    #print("Input matrix: \n" + str(input_nn))
    #print("Forward propagation: \n" + str(nn.forwardProp(input_nn)))
    #print("Actual data: \n" + str(output_of_data))

    # mean sum squared loss is the algorithm we use to initiate gradient descent
    # MSE tells the derivative where to move on the curve
    # gradient descent guides the movement to the local minimum by iterating MSE
    loss = np.mean(np.square(output_of_data - nn.forwardProp(input_nn)))
    lossArray.append(loss)
    print("Loss: \n" + str(loss))

    # we train on iteration 1, not 0 - i.e. network needs some base case before induction with training
    nn.train_network(input_nn, output_of_data)

print(lossArray)
