# code model:
# https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python

# d-types: https://docs.scipy.org/doc/numpy/reference/arrays.dtypes.html
# np.amax: https://docs.scipy.org/doc/numpy/reference/generated/numpy.amax.html
# np.random.randn: https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.random.randn.html
# np.transpose - https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.T.html

input_nn = np.array(factors[parameterStrings[parameter1]], dtype=float)
output_of_data = np.array(factors['Diagnosis'], dtype=float)

# scale units
input_nn = input_nn / np.amax(input_nn, axis=0) # amax returns the maximum of the array, axis=0 is x-axis
output_of_data = output_of_data / np.amax(output_of_data)

class neuralNetwork(object):
    def __init__(self):
        # parameters - amount of items in each layer
        # use 569 for amount of items because data sets have 569 floats, one for each patient
        self.inputSize = 569
        self.outputSize = 569
        self.hiddenSize = 569

        # use random.randn to return a random value from the provided standard normal curve

        # define weights with respect to their input & hidden sizes
        self.w1 = np.random.randn(self.inputSize, self.hiddenSize) # input to hidden
        self.w2 = np.random.randn(self.hiddenSize, self.outputSize) # hidden to output

    def forwardProp(self, X):
        self.z1 = np.dot(X, self.w1) # dot product (number) of input and first random weights
        self.z2 = self.sigmoid(self.z1) # activation function
        self.z3 = np.dot(self.z2, self.w2) # dot product of hidden layer and second random weights
        output = self.sigmoid(self.z3) # final activation function to return
        return output

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoidPrime(self, x):
        return x * (1 - x)

    def backProp(self, X, Y, output):
        # how different are the actual and predicted values?
        # use y - y-hat by convention
        self.outputError = Y - output

        # use gradient descent to move down the sigmoid curve
        # goal: get closer to absolute minimum, 0
        # use the derivative (change in slope) to describe a small, 'local' change on the grad descent algorithm
        self.outputDelta = self.outputError * self.sigmoidPrime(output)

        # np_array.T is the transpose of np_array
        self.z2_error = self.outputDelta.dot(self.w2.T)

        # same process of derivative and applying gradient descent for the hidden layer
        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)

        # how are we changing the weights?

        # take the transpose of the input array - X.T
        # take its dot product with the hidden layer delta (how is slope changing on the curve? i.e. gradient descent)
        # this dot product is the change in weights to get to 0 (absolute minimum)
        # transpose(X) * change in hidden layer slope = how the weight needs to change to get the algorithm to the min
        # this product is a change (or derivative), so it is added (not overridden) to the original weight
        self.w1 += X.T.dot(self.z2_delta) # input to hidden
        self.w2 += self.z2.T.dot(self.outputDelta) # hidden to output

    def train_network(self, X, Y):
        # implement what a neural network is - calculate, analyze error, repeat
        # this function runs in a loop so the network gets new information more than once
        output = self.forwardProp(X)
        self.backProp(X, Y, output)

nn = neuralNetwork()

for i in range(0, 6): # iterate five times as an arbitrary marker
    print("input: \n" + str(input_nn))
    print("Predicted: \n" + str(nn.forwardProp(input_nn)))
    print("Actual: \n" + str(output_of_data))

    # mean sum squared loss is the algorithm we use to initiate gradient descent
    # MSE tells the derivative where to move on the curve
    # gradient descent guides the movement to the local minimum by iterating MSE
    loss = np.mean(np.square(output_of_data - nn.forwardProp(input_nn)))
    print("Loss: \n" + str(loss))

    # we train on iteration 1, not 0 - i.e. network needs some base case before induction with training
    nn.train_network(input_nn, output_of_data)
    
