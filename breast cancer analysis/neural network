# code model:
# https://dev.to/shamdasani/build-a-flexible-neural-network-with-backpropagation-in-python

# define parameters for the neural network - two random factors + diagnosis
nn_parameter1 = factors[parameterStrings[parameter1]]
nn_parameter2 = factors[parameterStrings[parameter2]]
nn_parameter_output = factors['Diagnosis']

# too many elements increase computation time significantly (especially with itertools.product)
# conveniently decrease computation time by splitting into train and test sets
# train = 369, test = 200
n = 369

nn_parameter1_train = nn_parameter1[:n or None] # list splicing to get rid of all indices after n, or None if n = 0
nn_parameter1_test = nn_parameter1[:-n or None]

nn_parameter2_train = nn_parameter2[:n or None]
nn_parameter2_test = nn_parameter_output[:-n or None]

nn_parameter_output_train = nn_parameter_output[:n or None]
nn_parameter_output_test = nn_parameter_output[:-n or None]

"""
Approach:
Use itertools to create ordered pairs from two separate lists
e.g. a = [], b = [], itertools.product(a[i], b[j])
Do this for both train and test sets

Comments:
-Create empty arrays to append each ordered itertools pair
-Add a counter to break from the loop upon appending enough pairs, such that len(Y) = len(Y-hat) for the neural network
"""
input_nn_train = []
input_nn_test = []

# train
i_j_count = 0

# use the training set to establish the neural network's predictions
for i, j in itertools.product(nn_parameter1_train, nn_parameter2_train):
    input_nn_train.append([i, j]) # append ordered pair as a list, not a dictionary
    i_j_count += 1
    if i_j_count == 369: # based upon the prior n cut of the data: i_j_count + n = len(parameter) = 569
        break

# test, reset the value
i_j_count = 0

# prime the test set for prediction and printing
for i, j in itertools.product(nn_parameter1_test, nn_parameter2_test):
    input_nn_test.append([i, j]) # append for test set
    i_j_count += 1
    if i_j_count == 200:  # based upon the prior n cut of the data: i_j_count + n = len(parameter) = 569
        break

# no need to use counter for output data, because it is pre-split
output_of_data_train = []
output_of_data_test = []

# same process as train and test set for each parameter, but no need for the counter
for i in nn_parameter_output_train:
    output_of_data_train.append([nn_parameter_output_train[i]])

for i in nn_parameter_output_test:
    output_of_data_test.append([nn_parameter_output_test[i]])

# make the arrays np.arrays (which are essentially matrix arrays)
# this allows for matrix multiplication with dot products in the neural network
input_nn_train = np.array(input_nn_train)
input_nn_test = np.array(input_nn_test)

output_of_data_train = np.array(output_of_data_train)
output_of_data_test = np.array(output_of_data_test)

class neuralNetwork(object):
    def __init__(self):
        # parameters - amount of items in each layer (we use 569 to match the size of the data provided)
        self.inputSize = 2
        self.outputSize = 1
        self.hiddenSize = 3

        # use random.randn to return a random value from the provided standard normal curve

        # define weights with respect to their input & hidden sizes
        self.w1 = np.random.randn(self.inputSize, self.hiddenSize) # input to hidden
        self.w2 = np.random.randn(self.hiddenSize, self.outputSize) # hidden to output

    def forwardProp(self, X):
        self.z1 = np.dot(X, self.w1) # dot product (number) of input and first random 3x2 weights
        self.z2 = self.sigmoid(self.z1) # activation function
        self.z3 = np.dot(self.z2, self.w2) # dot product of hidden layer and second random 3x2 weights
        output = self.sigmoid(self.z3) # final activation function to return
        return output

    def sigmoid(self, x):
        return 1 / (1 + np.exp(-x))

    def sigmoidPrime(self, x):
        return x * (1 - x)

    def backProp(self, X, Y, output):
        # how different are the actual and predicted values?
        # use y - y-hat by convention
        self.outputError = Y - output

        # use gradient descent to move down the sigmoid curve
        # goal: get closer to absolute minimum, 0
        # use the derivative (change in slope) to describe a small, 'local' change on the gradient descent algorithm
        self.outputDelta = self.outputError * self.sigmoidPrime(output)

        # np_array.T is the transpose of np_array
        self.z2_error = self.outputDelta.dot(self.w2.T)

        # same process of derivative and applying gradient descent for the hidden layer
        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)

        # how are we changing the weights?
        # take the transpose of the input array - X.T
        # take its dot product with the hidden layer delta (how is slope changing on the curve? i.e. gradient descent)
        # this dot product is the change in weights to get to 0 (absolute minimum)
        # transpose(X) * change in hidden layer slope = how the weight needs to change to get the algorithm to the min
        # this product is a change (or derivative), so it is added (not overridden) to the original weight
        self.w1 += X.T.dot(self.z2_delta) # input to hidden
        self.w2 += self.z2.T.dot(self.outputDelta) # hidden to output

    def train_network(self, X, Y):
        # implement the neural network algorithm - calculate, analyze error, repeat
        # this function runs in a loop so the network gets new information more than once
        output = self.forwardProp(X)
        self.backProp(X, Y, output)

nn = neuralNetwork()

# create loss array to ensure that the network is more accurate with each iteration
lossArray = []

# train the network
for i in range(0, 5): # arbitrarily run 5 iterations
    # mean sum squared loss is the algorithm we use to initiate gradient descent
    # MSE tells the derivative where to move on the curve
    # gradient descent guides the movement to the local minimum by iterating MSE
    loss = np.mean(output_of_data_test - nn.forwardProp(input_nn_test))
    lossArray.append(loss)
    print("Loss " + str(i + 1) + ": \n" + str(loss))

    nn.train_network(input_nn_train, output_of_data_train)

print("Loss array: \n" + str(lossArray))
